---
title: "Residual analysis for community detection"
author: "David Selby"
date: "`r format(Sys.Date(), '%e %B %Y')`"
output:
  rmarkdown::html_vignette:
    df_print: kable
vignette: >
  %\VignetteIndexEntry{Residual analysis for community detection}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(fig.width = 6, fig.height = 5)

ggqqline <- function(vec, distribution = qnorm, probs = c(.25, .75), ..., na.rm = TRUE) {
  # Replicates stats::qqline in 'ggplot2'
  # NB: ggplot2 2.2.1.9000 offers stat_qq_line & geom_qq_line
  y <- stats::quantile(vec, probs, na.rm = na.rm)
  x <- distribution(probs, ...)
  slope <- diff(y) / diff(x)
  intercept <- y[1L] - slope * x[1L]
  ggplot2::geom_abline(intercept = intercept, slope = slope)
}
```

The **scrooge** package offers several utilities for diagnosing the output of community detection algorithms.

Consider this canonical example from Varin et al. (2016).
We have data describing the number of citations exchanged between 47 statistical journals.
Access it using `data(citations)`.

A standard hierarchical clustering of the data yields eight communities.

```{r clusters, echo = 1:3, results = 'asis'}
library(scrooge)
distances <- as.dist(1 - cor(citations + t(citations) - diag(diag(citations))))
clusters <- cutree(hclust(distances), h = 0.6)
cat(c('', sapply(split(names(clusters), clusters),
                       function(x) paste(x, collapse = ', ')
                 )
      ),
    sep = '\n1. ')
```

An 'expert' review of these clusters may confirm that they look sensible, but that's not good enough for us.
We want a *quantitative* diagnosis of their descriptive and predictive power.

Consider, for example, the journal *Biometrika*, which falls in the third cluster.
Is the citation behaviour of this publication described adequately by the community to which it has been assigned?

## Citation profiles

Every journal has a *citation profile*, which is a stochastic vector representing the distribution of that journal's outgoing citations.
It is the transition probability vector that describes one step of a random walk around the graph.

For example, suppose we have three journals, \(A\), \(B\) and \(C\).
If half of the references in journal \(C\)'s bibliography were to journal \(A\), a quarter to \(B\) and the remainder to itself, then journal \(C\)'s citation profile would be given by the vector \(\pmatrix{0.5 & 0.25 & 0.25}\).

Communities also have citation profiles, calculated from the citations of their constituent journals.
How these citations are aggregated is up to you---for now we settle for an unweighted sum, scaled to form a unit vector.

The eight communities in the statistical journals network each have a profile that lies somewhere on the part of the surface of the unit \(46\)-sphere in the positive orthant of \(\mathbb{R}^{47}\).

## Convex combinations of community profiles

However our clustering or community detection algorithm works, we have a grouping of journals into clusters.
Unless they are really homogeneous, this reduction in dimensionality throws away some detail of each journal's citation behaviour.

How well do the eight clusters describe *Biometrika*'s citation profile?

```{r}
Biometrika_profile <- citations[, 'Bka'] / colSums(citations)['Bka']
np <- nearest_point('Bka', citations, clusters)
near_Biometrika <- community_profile(citations, clusters) %*% np$solution
dissimilarity(Biometrika_profile, near_Biometrika)
```

Using the index of dissimilarity, we see that 16% of citations are misallocated, when comparing *Biometrika*'s profile to the nearest possible combination of the eight community profiles.

We can also compute Poisson residuals on the citation counts.

```{r residuals, message = FALSE}
Bka_predicted <- fitted_citations('Bka', citations, clusters)
Bka_residuals <- profile_residuals(Bka_predicted, citations[, 'Bka'])
```

Do the functions from the **scrooge** package calculate the residuals correctly?
Recall the formula for Poisson residuals is
\[
r_i = \frac{y_i - \hat{y}_i}{\sqrt{\hat{y}_i}}.
\]

The binary operator `%==%` allows us to check if two vectors are equal (whilst allowing for floating-point error).

```{r residual_test}
Bka_residuals %==% ((citations[, 'Bka'] - Bka_predicted) / sqrt(Bka_predicted))
```
How might we calculate this using a (null) `glm`?

Remember, a Poisson regression takes the form
\[
\log\mathbb{E[y]} = \mathbf{X}\boldsymbol\beta,
\]
so we will need to take logs whenever referring to predictors that are on the same scale as responses.

```{r residual_test_glm}
model <- glm(citations[, 'Bka'] ~ 0 + offset(log(Bka_predicted)), family = 'poisson')
Bka_residuals %==% resid(model, type = 'pearson')
```

Now that we have calculated some model residuals, we can plot them in the usual way.

```{r residual_plots}
suppressPackageStartupMessages(library(tidyverse))
theme_set(theme_bw())

resids_df <- data_frame(journal = rownames(Bka_residuals),
                        residual = drop(Bka_residuals),
                        fitted = drop(Bka_predicted),
                        q_theo = qqnorm(residual, plot = F)$x)

ggplot(resids_df) +
  aes(fitted, residual, label = journal) +
  geom_hline(yintercept = 0, colour = 'steelblue', linetype = 2) +
  geom_smooth(method = 'loess', colour = 'steelblue', se = FALSE) +
  geom_text(aes(colour = journal == 'Bka')) +
  labs(x = 'Predicted citation counts', y = 'Residuals',
       title = "Profile residuals for 'Biometrika'") +
  scale_colour_manual(values = c('black', 'tomato2'), guide = 'none')

ggplot(resids_df) +
  aes(q_theo, residual, label = journal) +
  geom_text(aes(colour = journal == 'Bka')) +
  ggqqline(Bka_residuals) +
  labs(x = 'Normal quantiles',
       y = 'Sample quantiles',
       title = "Profile residuals for 'Biometrika'") +
  scale_colour_manual(values = c('black', 'tomato2'), guide = 'none')
```

## Controlling for excessive self-citation behaviour

Notice how *Biometrika* has an extremely high profile residual for citing *itself*.
In other words, the clustering model is under-predicting self-citations.
To account for the possibility of 'excess' self-citations, we can add a term to the Poisson regression model.

```{r self_citation}
Bka_selfcites <- as.numeric(rownames(citations) == 'Bka')
# Bka_selfcites2 <- diag(diag(citations))[, rownames(citations) == 'Bka'] # actual count
Bka_selfcite_model <- update(model, . ~ . + Bka_selfcites)
broom::tidy(Bka_selfcite_model)
```
What do the residuals look like now?

```{r self_citation_residuals}
resids_self_df <- data_frame(journal = rownames(citations),
                             residual = resid(Bka_selfcite_model, type = 'pearson'),
                             fitted = fitted(Bka_selfcite_model),
                             q_theo = qqnorm(residual, plot = F)$x)

ggplot(resids_self_df) +
  aes(fitted, residual, label = journal) +
  geom_hline(yintercept = 0, colour = 'steelblue', linetype = 2) +
  geom_smooth(method = 'loess', colour = 'steelblue', se = FALSE) +
  geom_text(aes(colour = journal == 'Bka')) +
  labs(x = 'Predicted citation counts', y = 'Residuals',
       title = "Profile residuals for 'Biometrika'",
       subtitle = 'Controlling for excessive self-citation') +
  scale_colour_manual(values = c('black', 'tomato2'), guide = 'none')

ggplot(resids_self_df) +
  aes(q_theo, residual, label = journal) +
  geom_text(aes(colour = journal == 'Bka')) +
  ggqqline(Bka_residuals) +
  labs(x = 'Normal quantiles',
       y = 'Sample quantiles',
       title = "Profile residuals for 'Biometrika'",
       subtitle = 'Controlling for excessive self-citation') +
  scale_colour_manual(values = c('black', 'tomato2'), guide = 'none')
```

What can explain the departure of the top 9 or so journals from the Q--Q line?
Could it simply be the mismatch between the Poisson distribution and its Normal approximation?
Recall that a Poisson distribution may be approximated with Normal when \(\lambda\) is large.
Some journals' citation rates are larger than others!

The top 10 residuals for the *Biometrika* model are

```{r top10residuals}
resids_self_df %>%
  arrange(desc(residual)) %>%
  select(journal, residual) %>%
  top_n(10, residual)
```

Which journals are cited by Biometrika---or Biometrika's community---least?

```{r bottom10bka}
near_Biometrika %>%
  as.matrix %>% as.data.frame %>%
  rownames_to_column('journal') %>%
  rename(profile = 'V1') %>%
  arrange(profile)

citations[, 'Bka'] %>%
  as.data.frame %>%
  rename(citations = '.') %>%
  rownames_to_column('journal') %>%
  filter(citations > 0) %>%
  arrange(citations) %>%
  head(n = 10)
```

Nothing much there, it seems.

## Community residuals

Using the `community_residuals()` function from the **scrooge** package, we can calculate the residual sum of squares for each journal based on a given community structure.

```{r community_residuals}
community_resids <- data_frame(journal = rownames(citations),
                               articles = drop(scrooge::articles),
                               residual = community_residuals(citations, clusters),
                               outcitations = colSums(citations),
                               q_norm = qqnorm(residual, plot = F)$x,
                               q_chi = qchisq(ppoints(nrow(citations))[order(order(residual))], df = nrow(citations)))

ggplot(community_resids) +
  aes(outcitations, residual, label = journal) +
  geom_smooth(method = 'loess', se = F) +
  geom_text() +
  labs(x = 'Outgoing citations', y = 'Profile residual sum of squares',
       title = 'Community residuals against outgoing citation count')

ggplot(community_resids) +
  aes(articles, residual, label = journal) +
  geom_smooth(method = 'loess', se = F) +
  geom_text() +
  labs(x = 'Number of articles', y = 'Profile residual sum of squares',
       title = 'Community residuals against journal size')

ggplot(community_resids) +
  aes(q_norm, residual, label = journal) +
  geom_text() +
  ggqqline(community_resids$residual) +
  labs(x = 'Normal quantiles', y = 'Sample quantiles',
       title = 'QQ plot for community residuals (profile residual sum of squares)')

ggplot(community_resids) +
  aes(q_chi, residual, label = journal) +
  geom_text() +
  ggqqline(community_resids$residual, distribution = qchisq, df = nrow(citations)) +
  labs(x = 'Chi-squared quantiles', y = 'Sample quantiles',
       title = 'QQ plot for community residuals (profile residual sum of squares)')
```

Again, we may want to control for self-citation rates.

## Community residuals controlling for self-citation

Is there a scalable way of doing this without a loop fitting lots of glms?

We want

  - residual sum of squares for each model
  - where each model is either predicted citations
  - or predicted citations + self-citation component
  
Design matrix?

Compare the null glm:

```{r community_selfcites}
hat_citations <- fitted_citations(NULL, citations, clusters)
community_glm <- glm(c(citations) ~ 0 + offset(log(c(hat_citations))), family = poisson)
community_glm_resids <- resid(community_glm, type = 'pearson')
community_scrooge_resids <- profile_residuals(hat_citations, citations)

plot(c(community_scrooge_resids), community_glm_resids)
abline(a = 0, b = 1)
```

Now we want a glm with self-citation terms added.
Since the (log) predicted citations are all 'offsets', the design matrix is simply an identity matrix, with one term for each self-citation interaction.
When we already known the design matrix, we can use the function `glm.fit` rather than `glm`, as follows.

```{r glm_fit, eval = F}
# X <- Matrix::bdiag(
#   lapply(rownames(citations), function (x) as.numeric(rownames(citations) == x))
# )
# X <- as.data.frame(as.matrix(X))
# colnames(X) <- make.names(colnames(citations))
# X$y <- as.vector(citations)
# 
# # community_glm_selfcites <- glm.fit(x = as.matrix(X),
# #                                    y = y,
# #                                    offset = offset,
# #                                    family = poisson(),
# #                                    intercept = FALSE)
# frmla <- paste('y ~ 0 +', paste(make.names(colnames(citations)),
#                             collapse = '+'))
# community_glm_selfcites <- glm(
#   frmla,
#   data = X,
#   offset = log(c(fitted_community)),
#   family = poisson,
#   start = runif(47))

get_rss <- function(j) {
  expected <- fitted_citations(j, citations, clusters)
  mod <- glm(citations[, j] ~ 0 + as.numeric(rownames(citations) == j),
             offset = log(expected),
             family = poisson)
  sum(resid(mod, type = 'pearson')^2)
}

for (i in rownames(citations)) {
  message('Running model for ', i)
  get_rss(i)
  message('Model fitted without errors for ', i)
  message('----------')
}
```

1. Errors are thrown when fitting the model for *AmS*.
2. AoS gets a warning of `fitted rates numerically 0`
3. An error is thrown when fitting the model for *Bern*.

