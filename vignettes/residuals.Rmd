---
title: "Residual analysis for community detection"
author: "David Selby"
date: "`r format(Sys.Date(), '%e %B %Y')`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Residual analysis for community detection}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(fig.width = 6, fig.height = 5)
```

The **scrooge** package offers several utilities for diagnosing the output of community detection algorithms.

Consider this canonical example from Varin et al. (2016).
We have data describing the number of citations exchanged between 47 statistical journals.
Access it using `data(citations)`.

A standard hierarchical clustering of the data yields eight communities.

```{r clusters, echo = 1:3, results = 'asis'}
library(scrooge)
distances <- as.dist(1 - cor(citations + t(citations) - diag(diag(citations))))
clusters <- cutree(hclust(distances), h = 0.6)
cat(c('', sapply(split(names(clusters), clusters),
                       function(x) paste(x, collapse = ', ')
                 )
      ),
    sep = '\n1. ')
```

An 'expert' review of these clusters may confirm that they look sensible, but that's not good enough for us.
We want a *quantitative* diagnosis of their descriptive and predictive power.

Consider, for example, the journal *Biometrika*, which falls in the third cluster.
Is the citation behaviour of this publication described adequately by the community to which it has been assigned?

## Citation profiles

Every journal has a *citation profile*, which is a stochastic vector representing the distribution of that journal's outgoing citations.
It is the transition probability vector that describes one step of a random walk around the graph.

For example, suppose we have three journals, \(A\), \(B\) and \(C\).
If half of the references in journal \(C\)'s bibliography were to journal \(A\), a quarter to \(B\) and the remainder to itself, then journal \(C\)'s citation profile would be given by the vector \(\pmatrix{0.5 & 0.25 & 0.25}\).

Communities also have citation profiles, calculated from the citations of their constituent journals.
How these citations are aggregated is up to you---for now we settle for an unweighted sum, scaled to form a unit vector.

The eight communities in the statistical journals network each have a profile that lies somewhere on the part of the surface of the unit \(46\)-sphere in the positive orthant of \(\mathbb{R}^{47}\).

## Convex combinations of community profiles

However our clustering or community detection algorithm works, we have a grouping of journals into clusters.
Unless they are really homogeneous, this reduction in dimensionality throws away some detail of each journal's citation behaviour.

How well do the eight clusters describe *Biometrika*'s citation profile?

```{r}
Biometrika_profile <- citations[, 'Bka'] / colSums(citations)['Bka']
np <- nearest_point('Bka', citations, clusters)
near_Biometrika <- community_profile(citations, clusters) %*% np$solution
dissimilarity(Biometrika_profile, near_Biometrika)
```

Using the index of dissimilarity, we see that 16% of citations are misallocated, when comparing *Biometrika*'s profile to the nearest possible combination of the eight community profiles.

We can also compute Poisson residuals on the citation counts.

```{r residuals, message = FALSE}
Bka_predicted <- fitted_citations('Bka', citations, clusters)
Bka_residuals <- profile_residuals(Bka_predicted, citations[, 'Bka'])
```

Do the functions from the **scrooge** package calculate the residuals correctly?
Recall the formula for Poisson residuals is
\[
r_i = \frac{y_i - \hat{y}_i}{\sqrt{\hat{y}_i}}.
\]

```{r residual_test}
library(testthat)
all.equal(
  Bka_residuals,
  (citations[, 'Bka'] - Bka_predicted) / sqrt(Bka_predicted)
)
```
How might we calculate this using a (null) `glm`?

Remember, a Poisson regression takes the form
\[
\log\mathbb{E[y]} = \mathbf{X}\boldsymbol\beta,
\]
so we will need to take logs whenever referring to predictors that are on the same scale as responses.

**To do:** Build this into some unit tests

```{r residual_test_glm}
model <- glm(citations[, 'Bka'] ~ 0 + offset(log(Bka_predicted)), family = 'poisson')
all.equal(drop(Bka_residuals),
          resid(model, type = 'pearson'),
          check.attributes = FALSE)
```

```{r}
## Now accounting for self-citation:

# excess_selfcites <- diag(diag(citations))[, rownames(citations) == 'Bka']
# dummy_selfcites  <- as.numeric(rownames(citations) == 'Bka')
# selfcite_model  <- glm(citations[, 'Bka'] ~ predicted_citations + excess_selfcites - 1, family = 'poisson')
# selfcite_model2 <- glm(citations[, 'Bka'] ~ predicted_citations + dummy_selfcites  - 1, family = 'poisson')
# fitted(selfcite_model) - predicted_citations

suppressPackageStartupMessages(library(tidyverse))
theme_set(theme_bw())
resids_df <- data_frame(journal = rownames(Bka_residuals),
                        residual = drop(Bka_residuals),
                        fitted = drop(Bka_predicted),
                        q_theo = qqnorm(residual, plot = F)$x)

ggplot(resids_df) +
  aes(fitted, residual, label = journal) +
  geom_hline(yintercept = 0, colour = 'steelblue', linetype = 2) +
  geom_smooth(method = 'loess', colour = 'steelblue', se = FALSE) +
  geom_text(aes(colour = journal == 'Bka')) +
  labs(x = 'Predicted citation counts', y = 'Residuals',
       main = 'Profile residuals for \'Biometrika\'') +
  scale_colour_manual(values = c('black', 'tomato2'), guide = 'none')

ggqqline <- function(vec, distribution = qnorm, probs = c(.25, .75), ...) {
  # Replicates stats::qqline in 'ggplot2'
  # NB: ggplot2 2.2.1.9000 offers stat_qq_line & geom_qq_line
  y <- stats::quantile(vec, probs)
  x <- distribution(probs, ...)
  slope <- diff(y) / diff(x)
  intercept <- y[1L] - slope * x[1L]
  ggplot2::geom_abline(intercept = intercept, slope = slope)
}

ggplot(resids_df) +
  aes(q_theo, residual, label = journal) +
  geom_text(aes(colour = journal == 'Bka')) +
  scale_colour_manual(values = c('black', 'tomato2'), guide = 'none') +
  ggqqline(Bka_residuals) +
  labs(x = 'Normal quantiles',
       y = 'Sample quantiles',
       title = 'Profile residuals for \'Biometrika\'')
```

Using the `community_residuals()` function from the **scrooge** package, we can calculate the residual sum of squares for each journal based on a given community structure.

```{r community_residuals}
community_resids <- data_frame(journal = rownames(citations),
                               articles = drop(scrooge::articles),
                               residual = community_residuals(citations, clusters),
                               outcitations = colSums(citations),
                               q_norm = qqnorm(residual, plot = F)$x,
                               q_chi = qchisq(ppoints(nrow(citations))[order(order(residual))], df = nrow(citations)))

ggplot(community_resids) +
  aes(outcitations, residual, label = journal) +
  geom_smooth(method = 'loess', se = F) +
  geom_text() +
  labs(x = 'Outgoing citations', y = 'Profile residual sum of squares',
       title = 'Community residuals against outgoing citation count')

ggplot(community_resids) +
  aes(articles, residual, label = journal) +
  geom_smooth(method = 'loess', se = F) +
  geom_text() +
  labs(x = 'Number of articles', y = 'Profile residual sum of squares',
       title = 'Community residuals against journal size')

ggplot(community_resids) +
  aes(q_norm, residual, label = journal) +
  geom_text() +
  ggqqline(community_resids$residual) +
  labs(x = 'Normal quantiles', y = 'Sample quantiles',
       title = 'QQ plot for community residuals (profile residual sum of squares)')

ggplot(community_resids) +
  aes(q_chi, residual, label = journal) +
  geom_text() +
  ggqqline(community_resids$residual, distribution = qchisq, df = nrow(citations)) +
  labs(x = 'Chi-squared quantiles', y = 'Sample quantiles',
       title = 'QQ plot for community residuals (profile residual sum of squares)')
```
